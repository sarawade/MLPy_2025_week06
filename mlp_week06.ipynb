{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e64bacfd068b431196660916c066322f",
    "deepnote_cell_type": "markdown",
    "id": "6Og4DnJPrB4A"
   },
   "source": [
    "# Week 6 - Logistic Regression\n",
    "\n",
    "### Aims\n",
    "\n",
    "The main concepts covered in this notebook are:\n",
    "\n",
    ">* logistic regression\n",
    ">* performance metrics for classification\n",
    ">* dealing with imbalanced data \n",
    ">* multi-class logistic regression\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Binary Logistic Regression](#RBH)\n",
    "\n",
    "3. [Regularization](#SKV)\n",
    "\n",
    "4. [Imbalanced Data](#Imbal)\n",
    "\n",
    "5. [Multi-class Example](#mclog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fea000a8e8214acc9e127474d618496f",
    "deepnote_cell_type": "markdown",
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "This week we will be implementing logistic regression for a classification task. \n",
    "\n",
    "- We will mainly focus on the data set stored in `Default.csv`.\n",
    "- For the multi-class example, we consider the `iris data` at the end of the notebook.\n",
    "\n",
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. You should aim to switch roles between driver and navigator approximately every 15 minutes. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- In some Exercises, you will see some beneficial hints at the bottom of questions.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "512532ac466a4986b163d5eab35303cb",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "---\n",
    "\n",
    "# Setup <a id='setup'></a>\n",
    "\n",
    "## Packages\n",
    "\n",
    "Let's load the packages we need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cell_id": "1464112e8ecd4a3cbcec8a1f8c9c543a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2097,
    "execution_start": 1708691485245,
    "id": "grVNp8GrrH0g",
    "output_cleared": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Other necessary packages\n",
    "from sklearn.preprocessing import StandardScaler # scaling features\n",
    "from sklearn.pipeline import make_pipeline           # combining classifier steps\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cell_id": "cebfe2b8f4df434a9a6582b55143139b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1708691496102,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Plotting defaults, feel free to adjust if you need\n",
    "plt.rcParams['figure.figsize'] = (8,6)\n",
    "plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c6d33b652ee240339dc26eec55c9d6e1",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset collects information on **10000** individuals, recording whether they defaulted on their credit card or not as well as other characteristics. Specifically, the included columns in the data are:\n",
    "\n",
    "* `default` - Whether the individual has defaulted\n",
    "\n",
    "* `student` - Whether the individual is the student\n",
    "\n",
    "* `balance` - The balance in the individual's account\n",
    "\n",
    "* `income` - Income of an individual\n",
    "\n",
    "Our aim is to build a model using Logistic Regression to predict if person will default or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9f740535b98c40b1b4b32745fd4cdc7e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 347,
    "execution_start": 1708691506033,
    "id": "eF19U6ivvZ7r",
    "outputId": "fa3accef-409c-41d9-b61d-2ac6b5d8f4b6",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "df_default = pd.read_csv(\"Default.csv\", index_col=0)\n",
    "\n",
    "df_default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d9b1d260f63549ed9baf14bfd8d84c54",
    "deepnote_cell_type": "markdown",
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We will start with explanatory data analysis (EDA) to get more insight about the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0e762de7391e44248d5e993c0a16ba97",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "Examine the structure of the data. Consider the following questions:\n",
    "\n",
    "a. What are the types of each variable? Based on the descriptive statistics, do anticipate any feature engineering steps that may be needed?\n",
    "\n",
    "b. Are there any missing values in the data? \n",
    "\n",
    "c. Visualize the features (balance, income, student) and comment on any differences that you observe between individuals that have defaulted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3a229b7296cf4eb3957a4840e9eb73c2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 36,
    "execution_start": 1708691510550,
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Part a: info and descriptive statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3363604bb57347a58b6f2c2d12eb1e6f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 72,
    "execution_start": 1708691517086,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Part b: missing data check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c: Visualize the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5febe1cea2f2417da7b933f7c9ff3e28",
    "deepnote_cell_type": "markdown",
    "id": "01HS3aS8vZ7u"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Preprocessing the Data\n",
    "\n",
    "Next, let's create our feature matrix and response vector. We use `LabelEncoder` to encode our categorical output to a binary vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Feature matrix and response vector\n",
    "X, y = df_default.drop(['default'], axis=1), df_default['default']\n",
    "\n",
    "# Convert to numpy array\n",
    "X = X.values\n",
    "\n",
    "# Encode default\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Print the class distribution before splitting the data set \n",
    "print(pd.Series(y).value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the high class imbalance, with 96.67% of individuals that have not defaulted on their loan. Consider the following naive train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naively split the data into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle= True,\n",
    "                                                    test_size = 0.1, random_state=1112)\n",
    "\n",
    "\n",
    "# Check the proportion of defaults in both train and test data sets \n",
    "print(pd.Series(y_train).value_counts(normalize=True)*100)\n",
    "print(pd.Series(y_test).value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6dbb7f5bd56d475f9fc2219f3d40f83a",
    "deepnote_cell_type": "markdown",
    "id": "O6TMeCFgAJVt"
   },
   "source": [
    "### ðŸš© Exercise 2 (CORE)\n",
    "\n",
    "a. Why might you NOT want to use the train/test split above?\n",
    "\n",
    "b. Modify the code to have similar class proportions in the train and test sets. Print the class proportions to check if they are similar. \n",
    "\n",
    "**Hint:** consider the the additional argument `stratify=` inside of the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to stratify in terms of the response when spliting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9bc97fd5dc004385b71030e6fd531f01",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 59,
    "execution_start": 1708691526144,
    "source_hash": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a9baf3fbe56740a1ab24868d8d1305e3",
    "deepnote_cell_type": "markdown",
    "id": "_YZt5su8C7WZ"
   },
   "source": [
    "# Logistic Regression <a id='RBH'></a>\n",
    "\n",
    "Recall from our notes that for a binary output $y \\in \\lbrace 0, 1 \\rbrace$, **logistic regression** is a classifier that can be seen a simple generalization of linear regression by making two changes. \n",
    "\n",
    "- First, we replaced the **Gaussian** distribution of the output $y$ with a **Bernoulli** distribution. \n",
    "- Second, we pass the linear function of the inputs, $\\mathbf{w}^T\\mathbf{x}$, through a **link function** $g: \\mathbb{R} \\rightarrow [0,1]$. \n",
    "\n",
    "That is, we assume $y \\sim \\text{Bern}( g(\\mathbf{w}^T\\mathbf{x}))$.\n",
    "\n",
    "The link function takes values in the unit interval to ensure that the conditional probability of a success, \n",
    "\n",
    "$$p(y =1 \\mid \\mathbf{x}) = E[y | \\mathbf{x}] = g(\\mathbf{w}^T\\mathbf{x})$$\n",
    "\n",
    "is between zero and one. Specifically, in logistic regression, we select the **logistic** link function (S-shaped), defined as \n",
    "\n",
    "$$g(\\mathbf{w}^T\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T\\mathbf{x})}$$\n",
    "\n",
    "Putting these steps together, the logistic regression model is:\n",
    "\n",
    "$$y \\sim \\text{Bern}\\left( \\left[1 + \\exp(-\\mathbf{w}^T\\mathbf{x})\\right]^{-1} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f7c4601c53eb4d8da94275f477e99c1c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In `sklearn`, we can fit a logistic regression model using [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    "\n",
    "**Creating a logistic regression model:** some important options include\n",
    "- `penalty` specifies the type of regularization. Options are `l1`, `l2`, `elasticnet`, `None`. \n",
    "- `C` the inverse strength of the penalty parameter.\n",
    "- 'l1_ratio' the additional penalty parameter when using `elasticnet`.\n",
    "\n",
    "**CAUTION:** by default `LogisticRegression` uses an l2 regularization with penalty parameter `C=1`. This default should never be used! Instead, you should either set `penalty=None` or tune the value of `C` with cross-validation. \n",
    "\n",
    "**Fitting a logistic regression model:**\n",
    "- use the usual `.fit(X,y)` to fit the model. The fitted object stores the intercept and coefficients in the usual attributes `.intercept_` and `.coef_`.\n",
    "\n",
    "**Prediction:** methods include\n",
    "- `predict` which predicts the class label (either 0 or 1), \n",
    "- `predict_proba` which predicts the class probabilities, and \n",
    "- `predict_log_proba` which predicts the log probabilities of each class.\n",
    "\n",
    "Let's start by create a pipeline to for our logisitic regression model (with no penatly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "log_pipe = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [(\"cat\", OneHotEncoder(drop=[\"No\"]), [0]),\n",
    "         (\"num\", StandardScaler(), [1,2])]\n",
    "    ),\n",
    "    LogisticRegression(random_state=42, penalty=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8d7843e3ca08420294a54deb302f4be3",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "### ðŸš© Exercise 3 (CORE)\n",
    "\n",
    "- Fit the logistic regression model to the training data\n",
    "- Run the code to plot the coefficients and comment on their interpretation.\n",
    "- Compute the the accuracy score of the model on the testing data\n",
    "\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "You can use the `score` method of `LogisiticRegression` to compute the accuracy.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "07a314d1bb4d449db7894882b71c5828",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 217,
    "execution_start": 1708691535888,
    "id": "i2qXwFXvQbBq",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Fit the model using the training data\n",
    "\n",
    "\n",
    "# Compute accuracy on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with coefficents\n",
    "coefs = pd.DataFrame(\n",
    "    np.copy(log_pipe[1].coef_).T,\n",
    "    columns=[\"Coefficients\"],\n",
    "    index=df_default.columns[[1,2,3]],\n",
    ")\n",
    "\n",
    "# Plot the coefficients\n",
    "coefs.plot.barh(figsize=(9, 7))\n",
    "plt.title(\"Logistic regression\")\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.xlabel(\"Coefficient values\")\n",
    "plt.subplots_adjust(left=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models: Beyond Accuracy\n",
    "\n",
    "Because of the imbalanced nature of the data, looking at accuracy alone is misleading. Indeed, a naive classifier that predicts no one defaults would achieve a high accuracy of 96.7%.  \n",
    "\n",
    "A binary classifier can make two types of errors:\n",
    "\n",
    "- Incorrectly assigning an individual __who defaults__ to the __no default__ category (FN)\n",
    "- Incorrectly assigning an individual who __does not default__ to the __default__ category (FP)\n",
    "\n",
    "To better understand the FN vs FP tradeoff, we can compute the confusion matrix. Recall from our notes, the quantities reported in the confusion matrix are:\n",
    "\n",
    "\n",
    "$$\\text{TP} = \\sum_{n=1}^N \\mathbb{I}(y_n=1)\\mathbb{I}(\\widehat{y}_n=1),\\quad \\text{FP} = \\sum_{n=1}^N \\mathbb{I}(y_n=0)\\mathbb{I}(\\widehat{y}_n=1)$$\n",
    "$$\\text{FN} = \\sum_{n=1}^N \\mathbb{I}(y_n=1)\\mathbb{I}(\\widehat{y}_n=0), \\quad \\text{TN} = \\sum_{n=1}^N \\mathbb{I}(y_n=0)\\mathbb{I}(\\widehat{y}_n=0)$$\n",
    "\n",
    "\n",
    "where $y_n$ is the true class and $\\widehat{y}_n$ is the estimated class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE)\n",
    "\n",
    "1. Use [ConfusionMatrixDisplay.from_estimator](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator) to compute and visualize the confusion matrix. \n",
    "2. Comment on the importance of FNs compared to FPs to the credit card company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from our notes, the different evaluation measures can be defined from the confusion matrix, such as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+ \\text{TN}}, \\hspace{0.5cm} \\text{Recall (TPR)} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FN}} \\hspace{0.5cm}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FP}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2\\left(\\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\right) = \\frac{\\text{2TP}}{\\text{2TP + FP + FN}}\n",
    "$$\n",
    "\n",
    "To compute these quantites and more, functions are available in `sklearn.metrics`, including:\n",
    "\n",
    "1. [Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) (True Positive Rate)\n",
    "3. [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "4. [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "5. [AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "\n",
    "For a detailed list, see https://scikit-learn.org/1.5/modules/model_evaluation.html#classification-metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "Compute the accuracy, recall, precision, and F1-score on the test data for the fitted model (using the functions in `sklearn.metrics`) and comment on the model's performance.\n",
    "\n",
    "**Hint** you first need to use `.predict()` to predict the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8ec27961c3864631ac51a44209cae017",
    "deepnote_cell_type": "markdown",
    "id": "Szq6fbGf05A9"
   },
   "source": [
    "### ðŸš© Exercise 6 (EXTRA)\n",
    "\n",
    "Based on the confusion matrix computed below. Use the equations above to derive:\n",
    "\n",
    "1. False Positive Rate \n",
    "2. Recall\n",
    "3. Precision\n",
    "4. F1-score\n",
    "\n",
    "without using any additional built-in function from any module. If computed correctly, the numbers should match the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "abdeb9b7efdd4931aa7ff55b756fdc55",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 170,
    "execution_start": 1708691554834,
    "id": "yRdjYEgP1ahd",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict class labels\n",
    "y_test_pred = log_pipe.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "confmat = confusion_matrix(y_true = y_test, y_pred=y_test_pred)\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b523e2799f87485191fe4917e7e6ac82",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 228,
    "execution_start": 1708691560298,
    "source_hash": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "948ed6adda3a4aed918fb04b93980b6c",
    "deepnote_cell_type": "markdown",
    "id": "K7foBrtx5DXO"
   },
   "source": [
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "The handy function `RocCurveDisplay.from_estimator` is also available in `sklearn` for plotting  ROC curve for the fitted model. Use this function to plot the ROC curve and compute the AUC using `sklearn.metrics.roc_auc_score` on the test data. Comment on the model's performance based on these quantities.\n",
    "\n",
    "**Hint** you will need to use `predict_proba()` to compute the class probabilities for each data point, in order to compute the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a5fb5a10d8984b30a0af21e699f4279e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 126,
    "execution_start": 1708691565495,
    "id": "ClWZ7isN5ekk",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# Plot the ROC curve\n",
    "\n",
    "\n",
    "# Compute the AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3ce567f812dd49f6b73779a3db31ae57",
    "deepnote_cell_type": "markdown",
    "id": "ZqZpAT1MxolL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 8 (CORE)\n",
    "\n",
    "Now, use the function `PrecisionRecallDisplay.from_estimator` to plot the Precision-Recall curve for the fitted model on the test data. Comment on the model's performance based on this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fab4444bbde148878e699ec0cda6b9eb",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bedbc50976344cc385f7267b9da8de91",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Regularization <a id='SKV'></a>\n",
    "\n",
    "Regularization can be useful in logistic regression to deal with high-dimensional data and for variable selection. As previous stated `LogisiticRegression` offers four options through the 'penalty' parameter:\n",
    "- `penalty=None` corresponds to no regularization,\n",
    "- `penalty=l2` is the default and corresponds to l2 regularization (ridge),\n",
    "- `penalty=l1` corresponds to l1 regularization (lasso), and\n",
    "- `penalty=elasticnet` corresponds to a combination of l1 and l2 regularizaiton.\n",
    "\n",
    "For the three regularization methods, the parameter `C` represents the inverse strength of the penalty parameter. Additionaly, when using `elasticnet`, `l1_ratio` is an additional penalty parameter, controlling the balance between l1 and l2 regularization.\n",
    "\n",
    "Note: the choice of the algorithm (aka solver) depends on the penalty chosen (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details.)\n",
    "\n",
    "As we saw last week, it is important to tune the penalty parameter `C` (and `l1_ratio` if using elastic net) when including regularization. Before we start searching over hyperparameters, its worth noting that some of the folds may not have the same distribution of the classes. This is particulary important for imbalanced data and means the we could get a validation score that may be a poor estimate of performance (for example we may have a fold with very few positive classes or more than usual, which can cause large differences on imbalanced data). To address this,  when doing our grid search, we will use a `StratifiedKFold` to ensure the distribution of classes in our folds reflects the distribution in the larger data.\n",
    "\n",
    "Run the code below to investigate the difference in class distributions across folds when using `Kfold` vs `StratifiedKfold`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 5 folds\n",
    "KF = KFold(n_splits=5)\n",
    "SKF = StratifiedKFold(n_splits=5)\n",
    "\n",
    "fold_names = [\"KFold\", \"StratifiedKFold\"]\n",
    "for i, K in enumerate([KF, SKF]):\n",
    "    # Initialize an empty DataFrame to store counts for the current fold type\n",
    "    fold_nos = pd.DataFrame()\n",
    "    for j, (train_i, test_i) in enumerate(K.split(X_train, y_train)):\n",
    "        # Compute value counts for the current fold and ensure it's a DataFrame with appropriate columns\n",
    "        fold_no = pd.DataFrame(pd.Series(y_train[test_i]).value_counts()).T\n",
    "        fold_no.index = [\"Fold \" + str(j)]  # Rename the index to reflect the fold number\n",
    "        # Concatenate with the fold_nos DataFrame\n",
    "        fold_nos = pd.concat([fold_nos, fold_no], axis=0)\n",
    "    \n",
    "    fold_nos.fillna(0, inplace=True)  # Fill missing values with 0 if any class was not present in a fold\n",
    "    print(f\"{fold_names[i]} counts per fold:\\n\", fold_nos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's perform a grid search to tune the regularization parameter with l2 regularization. Notice that we have listed multiple metrics to save with the option `scoring=`. However, setting `refit=` will select the best model and refit according to the  specfied metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet sets up and executes a grid search for tuning the penalty parameter\n",
    "# of a logistic regression model with l2 regularization using cross-validation. \n",
    "\n",
    "# Pipeline \n",
    "log_pipe_l2 = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [(\"cat\", OneHotEncoder(drop=[\"No\"]), [0]),\n",
    "         (\"num\", StandardScaler(), [1,2])]\n",
    "    ),\n",
    "    LogisticRegression(random_state=42, penalty='l2')\n",
    ")\n",
    "\n",
    "# Uncomment this line to find how the penalty parameter is called in the pipeline\n",
    "#log_pipe.get_params()\n",
    "\n",
    "# Possible C values: \n",
    "C_list = np.linspace(0.01, 15, num=151)\n",
    "\n",
    "\n",
    "# Grid search CV:\n",
    "log_rs = GridSearchCV(log_pipe_l2, \n",
    "                      param_grid={'logisticregression__C': C_list},\n",
    "                      scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], #Evaluation metrics to compute on validation sets\n",
    "                      cv = StratifiedKFold(n_splits=5, shuffle=True),\n",
    "                      refit = \"accuracy\", # Refits the best model on the entire dataset using the accuracy metric \n",
    "                      return_train_score = True)\n",
    "\n",
    "# Tune the model with grid search:\n",
    "log_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 9 (CORE)\n",
    "\n",
    "a. Run the following code to plot the mean accuracy averaged across the validation folds (in black). Comment on the suggested value of `C`.\n",
    "\n",
    "b. Choose a metric other than accuracy (i.e. f1, recall, or precision) and redraw the figure to plot the mean across the validation folds. Does your suggest value of `C` change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only mean and split scores\n",
    "cv_accuracy = pd.DataFrame(\n",
    "    data = log_rs.cv_results_\n",
    ").filter(\n",
    "    # Extract the split#_test_accuracy and mean_test_accuracy columns\n",
    "    regex = '(split[0-4]+|mean)_test_accuracy'\n",
    ").assign(\n",
    "    # Add the alphas as a column\n",
    "    C = C_list\n",
    ")\n",
    "\n",
    "# Reshape the data frame for plotting\n",
    "d = cv_accuracy.melt(\n",
    "    id_vars=('C','mean_test_accuracy'),\n",
    "    var_name='fold',\n",
    "    value_name='Accuracy'\n",
    ")\n",
    "\n",
    "# Plot the validation scores across folds\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(x='C', y='Accuracy', color='black', errorbar=None, data = d)  # Plot the mean score in black.\n",
    "sns.lineplot(x='C', y='Accuracy', hue='fold', data = d) # Plot the curves for each fold in different colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redraw figure for a different metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e90ea58d75434a84a19170ba7a2e6a6d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### ðŸš© Exercise 10 (EXTRA)\n",
    "\n",
    "a. Rebuild your model with `l1` regularization and perform a grid search to tune the penalty parameter. Which metric have you chosen for refitting the model and why?\n",
    "\n",
    "b. Plot the coefficients. Which variables are included? How does the performance compare to the model with no regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1854957109fa4402a3cd0051ebbe1eec",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 35098,
    "execution_start": 1708691672333,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Model with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics on the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "303c5d369e994e639fd621fea2f790f1",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c2a4f8eb53a54b80b47fef9cd93a6409",
    "deepnote_cell_type": "markdown",
    "id": "Pew94RYnTVZh"
   },
   "source": [
    "# Dealing with Imbalanced Data <a id='Imbal'></a>\n",
    "\n",
    "We have already seen that when dealing with imbalanced data, we need to \n",
    "1. Considering a suitable performance metric both for testing and validation.\n",
    "2. Use a suitable splitting strategy (stratified splitting) both in creating our test and validation sets.\n",
    "\n",
    "But the majority class can still overwelm the minority class when fitting the model. To investigate, let's use the function  [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), which reports summary of the precision, recall, F1 score for each class. Note that:\n",
    "- the recall of the positive class is also known as *sensitivity*; recall of the negative class is *specificity*.\n",
    "- it also includes the *macro average* (averaging the unweighted mean per label) as well as the weighted average (averaging the support-weighted mean per label). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cfbf0e6cd6524d52b4f75ee8f8024867",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 94,
    "execution_start": 1708691714027,
    "id": "53D2HcEQjec4",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Compute and print the classification report for the model with no penatly\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, \n",
    "                            log_pipe.predict(X_test), \n",
    "                            target_names = ['No Default','Default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, how the metrics are much higher for the majority class (individuals who have not deafulted). In this case, we may want to alter the training algorithm or data by:\n",
    "\n",
    "1. Weighting the classes during training.\n",
    "2. Resampling the data.\n",
    "\n",
    "`LogisticRegression` offers the option to include class weights, and for example `class_weight=\"balanced\"` will weight the classes by:\n",
    "$$ \\frac{N}{2 N_c}, \\text{ where } N_c \\text{ for } c = 0, 1 \\text{ counts the number of observations in each class.}$$ \n",
    "\n",
    "In the following, the main focus is on suitable **resampling** to change the distribution of the classes in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "753b0a0ced804206b4e5f0e54c9eb862",
    "deepnote_cell_type": "markdown",
    "hideCode": false,
    "hidePrompt": false,
    "id": "WMdQ_gVdjec5"
   },
   "source": [
    "## Resampling\n",
    "\n",
    "To alter the distribution of the classes in our training data, there are two main approaches:\n",
    "\n",
    "- Under-sampling the majority class\n",
    "- Over-sampling the minority class\n",
    "\n",
    "We will be using Imbalanced-learn (imported as `imblearn`), an open source, MIT-licensed library relying on scikit-learn that provides tools for dealing with classification with imbalanced classes (see [here](https://imbalanced-learn.org/stable/introduction.html) for an introduction). Let's start by installing the `imblearn` package if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "cell_id": "13bd3916dd744409812d124090a7c1bc",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2774,
    "execution_start": 1708691964044,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Install the imblearn if necessary \n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks, we learned about _transformers_, which allow us to __alter the features__ and not the number of observations in our data (i.e. the columns of $\\mathbf{X}$). Instead, _resamplers_ provide a preprocessing step in our `Pipeline` to __alter the number of observations__ and not the features (i.e. the rows of $\\mathbf{X}$ and $\\mathbf{y}$).\n",
    "\n",
    "__Resamplers__\n",
    "\n",
    "Resamplers are classes that follow the scikit-learn API and have a sampling functionality through the `.resample()` method. Like all other scikit-learn methods, they have a `.fit()` method which is only applied during pipeline training. This means if we want to create our own resampler from scratch that is compatible with scikit-learn, we just have to make a class that has three methods; `.fit()`,  `.resample()`, and `.fit_resample()`; with the latter just chaining the other two together.\n",
    "\n",
    "Therefore to resample a dataset, each sampler implements:\n",
    "\n",
    "```\n",
    "obj.fit(data, targets)\n",
    "data_resampled, targets_resampled = obj.resample(data, targets)\n",
    "```\n",
    "\n",
    "or simply...\n",
    "\n",
    "```\n",
    "data_resampled, targets_resampled = obj.fit_resample(data, targets)\n",
    "```\n",
    "\n",
    "**Remember to include your sampler within your model pipeline to prevent data leakage!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cceced5195fa4c2e85573409a672256a",
    "deepnote_cell_type": "markdown",
    "id": "qctA7g8tZH_C"
   },
   "source": [
    "\n",
    "### Under-Sampling\n",
    "\n",
    "Under-sampling involves removing observations from the majority class to prevent its signal from dominating during training. We will focus on the simplest strategy:\n",
    "\n",
    "- [`RandomUnderSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html), which randomly removes observations from the majority class.\n",
    "\n",
    "For an overview of other under-samplers, see: https://imbalanced-learn.org/stable/under_sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9c2e33bd2e794deeae5f4e03c70438e0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 30192,
    "execution_start": 1708691969427,
    "hideCode": false,
    "hidePrompt": false,
    "id": "Pa8blBDVjec6",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Pipeline with under sampling\n",
    "log_pipe_us = ImPipeline([\n",
    "    (\"fe\",ColumnTransformer(\n",
    "        [(\"cat\", OneHotEncoder(drop=[\"No\"]), [0]),\n",
    "         (\"num\", StandardScaler(), [1,2])]\n",
    "    )),\n",
    "   (\"sampler\", RandomUnderSampler(random_state=42)),\n",
    "   (\"model\", LogisticRegression(random_state=42, penalty=None))])\n",
    "\n",
    "# Fit model     \n",
    "log_pipe_us.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "Compute and print the classification report and visualize the confusion matrix for the model above with undersampling. How have the results changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4ce88cdf88364959ac66dd11ba7589ce",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 260,
    "execution_start": 1708692003643,
    "id": "NdzVhFDNjec6",
    "source_hash": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73e5aebefbfb46d98f385c256fc439a2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38ecfccc67ba42dba791f12a802a7c17",
    "deepnote_cell_type": "markdown",
    "hideCode": false,
    "hidePrompt": false,
    "id": "1xk6k_W-jec6"
   },
   "source": [
    "## Oversampling\n",
    "\n",
    "Over-sampling involves generating additional observations from the minority class. We will focus on the simplest strategy:\n",
    "\n",
    "- [`RandomOverSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html), which randomly samples with replacement observations from the minority class.\n",
    "\n",
    "For an overview of other over-samplers, see: https://imbalanced-learn.org/stable/over_sampling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a1eba9ae488f4107b8ac47797d20e0bf",
    "deepnote_cell_type": "markdown",
    "id": "4qQM8VKxAybZ"
   },
   "source": [
    "### ðŸš© Exercise 12 (CORE)\n",
    "\n",
    "a. Create a new pipeline for a logisitic regression model with oversampling (by copying and editting the pipeline above).\n",
    "\n",
    "b. How do the performance metrics compare with under-sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6626d87322d547ecafddc6061e0760de",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 72745,
    "execution_start": 1708692014155,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f50ee641e8b94b35b781fe8c2bea31b0",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2c3faebe63a64a4591397d38658277e5",
    "deepnote_cell_type": "markdown",
    "id": "ilhDv3WHvZ73"
   },
   "source": [
    "---\n",
    "# Multi-class Logistic Regression <a id='mclog'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a53598273a3e475bb5ea89a71d34b209",
    "deepnote_cell_type": "markdown",
    "id": "NtUQzMD9vZ73"
   },
   "source": [
    "Finally, to gain some experience with multi-class logistic regression, let's now look at the `iris` data set, which we have seen already in lectures.  \n",
    "\n",
    "This is an example where the response has **3 classes** corresponding to the three types of iris species and $D=4$ features (petal length, petal width, sepal length, sepal width).\n",
    "\n",
    "Let's start by loading the data and doing some basic EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the data \n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Loading data\n",
    "iris = pd.DataFrame(sns.load_dataset('iris'))\n",
    "\n",
    "# Print information about the data set\n",
    "print(iris.info())\n",
    "print(iris.describe())\n",
    "print(iris['species'].value_counts())\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data = iris, hue = 'species',corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "83819a1d373a4c32a41faf10e8428820",
    "deepnote_cell_type": "markdown",
    "id": "kMZfWvLKvZ74"
   },
   "source": [
    "From the intial EDA, we observe:\n",
    "- The features have similar, but slightly different scales.\n",
    "- Visually, the species appear to be fairly well separated.\n",
    "- The target variable `species` is a string and we will need a label encoding\n",
    "- The species types are equal distributed, and thus no methods for class imbalance are needed.\n",
    "\n",
    "Now, let's separate the features and target, encode the target, and split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cell_id": "f3a088f39d5c4dc5a32be93a161372a1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1708692201179,
    "id": "VcuBnGPavZ73",
    "outputId": "6e1a6cad-acb2-454e-ea14-4e1753f4d69e",
    "output_cleared": true,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Feature matrix and response vector\n",
    "X, y = iris.drop(['species'], axis=1), iris['species']\n",
    "\n",
    "# Convert to numpy array\n",
    "X = X.values\n",
    "\n",
    "# Encode target\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Split into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 13 (EXTRA)\n",
    "\n",
    "We can use the same `LogisticRegression` for the multi-class setting. In this case, it will fit the **multinomial logistic regression** model. Fit a multinomial logisitic regression model and visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 14 (EXTRA)\n",
    "\n",
    " Some machine learning classification algorithms are only suited to binary classification. The **One-vs-Rest** scheme is simple strategy to extend any binary classification method to the multi-class setting, by simply:\n",
    " - one-hot encoding the target variable, and\n",
    " - fitting at binary classification model for each class against all other classes. \n",
    " \n",
    "This can be easily implemented using `sklearn`'s [OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html).\n",
    "\n",
    "Fit a one-vs-rest logistic regression model and visual the confusion matrix. How do the results compare with the multinomial model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "be8c80aa530f46ef84916e3177571a26",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 26,
    "execution_start": 1708692269820,
    "id": "c3mFzKbLCI_J",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5508829b9bc64258a19a67afb60856e8",
    "deepnote_cell_type": "markdown",
    "id": "EW10NyWz8vK-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c5487cb181524c3d8e0b6d582cd8a4c7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7b69abf2640a44fbaa1fa949ae62d4bc",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week06_key.ipynb "
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d2dd3d1e7ef346c7a6857bfb9e77931f",
  "deepnote_persisted_session": {
   "createdAt": "2024-02-23T13:00:34.679Z"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
